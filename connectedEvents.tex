% THIS IS SIGPROC-SP.TEX - VERSION 3.1
% WORKS WITH V3.2SP OF ACM_PROC_ARTICLE-SP.CLS
% APRIL 2009

\documentclass{acm_proc_article-sp}

\newcommand{\superscript}[1]{\ensuremath{^{\textrm{#1}}}}
\def\sharedaffiliation{\end{tabular}\newline\begin{tabular}{c}}

\def\wu{\superscript{1}}
\def\wg{\superscript{2}}



\usepackage{url}
\usepackage{color}
\usepackage{multirow}
\usepackage{mathtools}

\begin{document}

%\title{Context-aware Connections between News Events}
\title{Describing and Contextualizing News Events in TV}
%\subtitle{Exploring DBpedia paths between named entities belonging to different event contexts}

\numberofauthors{5} 

\author{
\alignauthor
Laurens De Vocht\wu
       \affaddr{\email{\texttt{laurens.devocht@ugent.be}}}
\and
\alignauthor
Erik Mannens\wu
       \affaddr{\email{\texttt{erik.mannens@ugent.be}}}
\and
\alignauthor
Rik Van de Walle\wu
       \affaddr{\email{\texttt{rik.vandewalle@ugent.be}}}
% 3rd. author
\and
\alignauthor 
Rapha\"el Troncy\wg
	\affaddr{\email{\texttt{raphael.troncy@eurecom.fr}}}
\and
\alignauthor 
Jos\'e Luis Redondo Garc\'ia\wg
	\affaddr{\email{\texttt{redondo@eurecom.fr}}}
\sharedaffiliation
\begin{tabular}{ccc}
    \affaddr{{\wu}Ghent University - iMinds - Multimedia Lab{\ }} & & 
    \affaddr{{\wg}EURECOM{\ }} \\
    %\affaddr{Gaston Crommenlaan 8/201} & \affaddr{} \\
    \affaddr{Ghent, Belgium} & &
    \affaddr{Biot, France} \\
\end{tabular}
}
%USED SHARED AFFILIATIONS TO SAVE SPACE

\maketitle
\begin{abstract}
There exist many approaches tackling the challenge of finding relevant entities to a document, but : 
(i) there are not mentioned;
(ii) there are not ranked
(iii) there are not related.


%TODO: Can you explain these a bit more?
In this paper we combine the power of non-structured resources with structured resources of DBpedia. Many applications and users leverage these entities to construct and link to other media.
We demonstrate that ... by explaining a use case about ...
%TODO: complete after the use case is written out.

\end{abstract}

% A category with the (minimum) three required fields
%\category{H.4}{Information Systems Applications}{Miscellaneous}
%A category including the fourth, optional field follows...
%\category{D.2.8}{Software Engineering}{Metrics}[complexity measures, performance measures]

%\terms{Theory}

%\keywords{ACM proceedings, \LaTeX, text tagging} % NOT required for Proceedings

\section{Introduction}

Online media is increasing in scale and ubiquity, but it is currently still unstructured and not good connected to media of other forms or from other sources.
[State of the Art: Named Entity Extraction, Expansion]
First attempts for extracting named entities out of media, which is a more and more common practice, give promising results but there it introduces some issues.
%TODO: Go deeper in these issues: probably related to those mentioned in the abstract.
LinkedTV is an integrated and practical approach towards experiencing Networked Media in the Future Internet. Within LinkedTV we want to connect media based on extracted entities that we link to DBpedia resources.

[More about LinkedTV?]
%TODO: Add here more info about the project

%[State of the Art: Everything is Connected]
We use therefore an optimized pathfinding algorithm \cite{de2013discovering} implemented in the Everything is Connected Engine (EiCE), firstly introduced with the Everything is Connected (EiC) Demo at the \emph{ISWC`12 Boston} conference.
Applying these algorithms to Linked Data can facilitate the resolving of complex queries that involve the semantics of the relations between resources. This method makes it is possible not only to discover relevant resources but also to filter them adapted to a specific context. Users can control and define which kinds of connections and types of resources really matter. Each path path between the discovered resources has a semantic meaning can be traced back to the original configuration of the user and
forms the basis of an explanation rather than a ranking. A major contribution in this approach is that it minimizes the size of the candidate pool of nodes in order to optimize queries and increase
the quality of the resulting paths.

\section{The Approach}

The approach described in this paper intends to reconstruct the semantic context associated to one particular news video, highlighting the main concepts and entities involved and explaining how they are related to each other. The complete processing workflow takes as input the textual transcript of a multimedia resource depicting the event, as well as the start and end date for which that particular event has been considered relevant. If the event is still ongoing we consider the current day as the end of the temporal interval. The corresponding video providers and platforms make sure this data is available beforehand.

In addition, we assume that the analyzed event has a minimum presence and coverage in the Web in order to ensure that the subsequent data mining techniques can collect an enough quantity of data to reconstruct the event's context. The output of the algorithm is a pair $Context_{Event1}=\left [  \varepsilon , \rho \right ]$  where $\varepsilon$ is a list named entities together with a numeric relevance score ($ \varepsilon =\left \{ E\times \mathbb{R} \right \}$,  being $E$ named entities classified using the NERD ontology\footnote{\url{http://nerd.eurecom.fr/ontology/nerd-v0.5.n3}}) and $\rho$ is a set of predicates $\left [ e_{1},e_{2}, p \right ]$, relating the aforementioned entities ($e_{1}\in E \vee  e_{2}\in E$). 

Our hypothesis states that this knowledge representation of the events can provide a sufficient source of information for satisfying the viewer's information needs and supporting complex multimedia operations such as search and hyperlinking. 


\subsection{Named Entity Extraction}

For each news item we perform named-entity recognition over the corresponding subtitles by using the NERD~\cite{Rizzo2012b} framework. The language of the considered videos is English but NERD supports many other languages so our approach is applicable in other languages. The output of this phase is a collection of entities annotated using the NERD Ontology v0.5, that come with a first relevance score obtained from the considered extractors. This set includes a first set of ranked entities that are explicitly mentioned during the video. Other entity based video annotation tools~\cite{yunjia2013} stop at this point even when there are still missing entities that can be relevant for the viewer in the context of the current event. We will try to alleviate this problem by extending this first list of concepts via the entity expansion component.

\subsection{Named Entity Expansion from Unstructured Resources}

The set of entities obtained from a traditional name entity extraction operation is normally insufficient and incomplete for expressing the context of a News event. For example, many entities spotted over a particular document are not finally disambiguated because the textual clues around the term do not allow the name entity extractor to calculate it. In other cases some entities are not even mentioned in the transcript but still are relevant for understanding the story behind the depicted entities. This is an inherent problem in information retrieval tasks: a single description about the same resource does not necessarily summarize the whole picture behind it.

The named entity expansion operation relies in the idea of retrieving and analyzing extra documents in the Web where the same event or the same part of the event is also described. By increasing the size off the sample we can increase the completeness of the context and the representativeness of the proposed set of entities. In other words, it is possible to reinforce relevant entities, and find new ones that could be also potentially interesting inside the context of that particular news item. 

The entire logic will be further described in the following subsections and consist mainly in (1) building an appropriate search query out of the original set of entities, (2) retrieving extra documents about the same news event, and (3) analyzing them for providing a more complete and better ranked set of final entities, as illustrated in Figure~\ref{fig:namedEntityExpansion}. 

\begin{figure}[h!]
\centering
\includegraphics[width=0.4\textwidth]{figure/ExpansionDiagram}
\caption{Schema of Named Entity Expansion Algorithm.}
\label{fig:namedEntityExpansion}%\end{figure}
\end{figure}

\subsubsection{Query Generation}

The Five Ws is a popular concept in information gathering in journalistic reporting. It captures the main aspects of a story or incidence: who, when, what, where, and why~\cite{LiJia2007}. In this paper we try to represent the news item in terms of four of those five W's (who is involved in the event, where the event is taking place, what the event is about, and when has happened) in order to generate a query that retrieves documents having to with the same event.This is somewhat like what a normal user would tape in a Web search engine when looking for descriptive documents about the same news item.  

Firstly the original entities are mapped to the NERD Core ontology, which considers 10 main classes: Thing, Amount, Animal, Event, Function, Organization, Location, Person, Product and Time. From this ten categories we generalize into three: the Who from \url{http://nerd.eurecom.fr/ontology#Person} and \url{http://nerd.eurecom.fr/ontology#Organization}, the Where from \url{http://nerd.eurecom.fr/ontology#Location}, and the What from the rest of NERD types after discarding \url{http://nerd.eurecom.fr/ontology#Time} and \url{http://nerd.eurecom.fr/ontology#Amount} instances. The When or so-called temporal dimension does not need to be computed since it is provided by the video publisher as an inputs for this approach.

Having the three set of entities already generated, the next step consist in ranking them in relevance according to a weighted sum of two different dimensions: their frequency in the transcripts and their former relevance scores coming from the NE extractors. We have defined the function $\text{filterEntities}(S)$ for selecting the $n$ entities inside the set of entities $S$ whose relative relevance $R_{rel}\left ( e_{i}, S \right ) = R\left ( e_{i} \right ) / Avg \left ( R\left ( e_{i} \right )  \right )$ falls into the upper quarter of the interval $\left [ max\left ( R_{rel}\left ( e_{i}, S \right )  \right ) -min\left (  R_{rel}\left ( e_{i}, S \right ) \right ) \right ]$. The final query is a pair $\text{Query}_{Event} =\left [ \text{textQuery}, t \right ]$ where \textit{textQuery} is the result of concatenating the labels of the most relevant entities in the sets Who, What, Where in that particular order $\textit{textQuery} = \text{labels}\left ( \text{filterEntities}(Who)  + \text{filterEntities}(What) +\text{filterEntities}(Where)  \right )$, and $t$ the time period dimension. 
This query generation process is depicted in the upper part of . For future research in the query construction technique we plan to directly use the Web resources themselves and not only their labels. 

\subsubsection{Document Retrieval}

Once $\text{Query}_{Event}$ is built out of the original set of named entities it will be ready to be injected into a document search engine where additional descriptions about the news event can be found. In this situation, the kind of query generated in the previous step and the search engine chosen should be closely tied in order to maximize the quality of the obtained results. Of course the different behavior of different search engines make some alternatives more suitable than others for certain kinds of events. The way the resultant documents change along the different search engines for a particular kind of event is a research question that will not be studied in this paper.

For the work presented in this paper we have resorted to Google Search REST API service available at \url{http://ajax.googleapis.com/ajax/services/search/web?v=1.0} by launching a query with the text \textit{textQuery}. Due to quota restrictions imposed by Google, the maximum number of retrieved document is set to 30. However and according to the evaluation performed in Section~\ref{sec:evaluation} this number is already enough for significantly extending the initial set of entities directly spotted by NERD. 

In which concerns to the temporal dimension, we keep only the documents published inside the time period $t+t_{e}$. We increase the original event period in $t_{e}$ is because documents concerning a news event are not always published during the time the action is taking place but some hours or days after. The value of $t_{e}$ depends on many factors like the nature of the event itself (it is a quick appearance in the media, or a deep fact with more repercussion) or the kind of documents the search engine is indexing (from very deep and elaborated documents that need time to be published, to light post generated by users in just some minutes). Based on the simple assumption that that longer event will provoke longer buzzes, we approximated  $t_{e} = t$, meaning that we consider also document published duration of the event. 

In the middle part of Figure~\ref{fig:namedEntityExpansion} this process is shown. The query is inputted in the search engine for obtaining other documents that elaborate on the the same event mentioned in the original video.Those documents (colored in black in the figure) will be also further processed for increasing the size of the collection and getting extra insights about the the studied news item.

\subsubsection{Entity Clustering}

In this phase the extra documents previously retrieved are now preprocessed and analyzed in order to extend and rerank the original set of relevant entities and consequently get more insights about the studied event. First, since most of the retrieved resources are Web pages, HTML tags and other annotations are removed, keeping only the main textual information. This plain text is then analyzed by the NERD framework in order to extract the named entities present in them.

In order to calculate the frequency of a certain resource inside the entire corpora, we need to group the different appearances of the same instance and check their cardinality. However, this is not a trivial task since the same entity can appear under different text labels, have typos in the field or different disambiguation URL's pointing to the same resource. For this reason, we have performed a centroid-based cluster operation over the instances of the entities. We have considered the centroid of a cluster the entity with the most frequent disambiguation URL that have also the most repeated label. As distance for comparing pairs of entities we have applied strict string similarity over the URL's, and in case of mismatch, Jaro–Winkler string distance~\cite{Winkler06overviewof} over the labels.The output of this phase is a list of clusters containing different instances of the same entity.

\subsubsection{Entity Ranking}

The final step of the expansion consists of ranking the different named entities obtained so far. In order to create this ordered list a score has been assigned to every entity according to the following features: relative frequency of the entity in the transcripts of the event video, relative frequency of the entity over the extra document, and average relevance for the different instances according to the named entity extractors. The three dimensions are combined via a weighted sum where the relative frequency in the video subtitles has a bigger impact in the final score, followed by the relative frequency on the searched documents, and ending with the average relevance from the extractors that is normally very variable.

The final output of the entity expansion operation is a list of entities (normally filtered by a minimum number of appearances since the method brings a much higher number of candidate entities), together with their ranking score and the frequency in both the main video and in the collected extra documents.

$\text{EntityExpansion}\left (subtitles, timePeriod \right )= \left ( \left \{ e_{i},  relScore_{i}, Freq\left ( subtitles, e_{i} \right ), Freq\left ( extraDocs, e_{i} \right ) \right \} \right )$

Entities with higher $relScore_{i}$ in the final classification are the more representative for the context of the original. Also there are other advantages in the final set of results: 
\begin{itemize}
  \item The entities will be better ranked since the size of the sample is bigger. Those entities that have appeared repeatedly in the extra documents will be promoted while those who barely appear in two or three will have less relevance.
  \item Entities that originally have not been disambiguated can have now their corresponding URL if any of the similar instances that appear in the additional documents can provide a link to a Web resource. The same occurs with labels that contained errors or were uncompleted. 
  \item  Finally, some entities that were important in the context of the news item but were not even mentioned in the original transcripts are now included in the list of relevant items since they have been often spotted over the collected documents. 
\end{itemize}


\subsection{Refining the Event Context via DBPedia Knowledge}

Once the set of context relevant entities has been expanded, we will use knowledge present in structured sources (DBpedia) for reinforcing the important entities and find relevant predicates between them. 

\subsubsection{Generating DBpedia paths}

Before we filter the relations between resources, the candidate resources to be included in relations are being pre-ranked. They are pre-ranked according to ``popularity'' and ``rarity'' essential components in the original PageRank algorithm \cite{page1999pagerank} and is used to sort candidate related nodes in the EiCE. The implementation of the EiCE takes the relations in to account by making use of the Jaccard coefficient to measure the dissimilarity and assign random walks based weight able to highly rank more rare resources, guaranteeing that paths between resources prefer specific relations and not general ones \cite{moore2012novel}.

\begin{figure}[h!]
\centering
\includegraphics[width=0.4\textwidth]{graphmultipatternmatching.png}
\caption{Pattern matching with multiple results using an iterative pathfinding process.}
\label{fig:patternmatching}
\end{figure}

We pass on the main start resource, a target and via points. Figure~\ref{fig:patternmatching} shows the iterative process for generating DBPedia paths. An initial state is computed in step \ref{fig:patternmatching}a. There are low weights and high weights, for the example 1 and 2 respectively. Based on the weights of the links te path through the vias is optimized, so a path with the lowest total weight will be selected first, until the vias are added to the exclude list. The path from start to end is forced through the given via points (\ref{fig:patternmatching}b). This leads to additional visited resource as via points (\ref{fig:patternmatching}c). They occur because to resolve each path, the route is being computed starting in the start resource, target and the via points. The resources where they converge to each other are considered as the new via points. These via points are included in the paths and therefore marked as visited. This is to make sure that in the next iteration paths will go around the visited via (\ref{fig:patternmatching}d). The next paths are being computed over and over (\ref{fig:patternmatching}e) until a threshold number of paths is found , and the context is large enough or when it takes to long to compute the next path (out of range). The final set of optimized paths used to for the context expansion.

\subsubsection{Analyzing Context Relevant Paths}

Entities are divided in two groups: main entities (\textit{MainEntities}, entities with relative score higher than the average), and additional entities (\textit{additionalEntities}, the rest.).
We applied the DPpedia path finding technique implemented in \url{demo.everythingisconnected.be/relations?uris=} over all the possible permutations of the set $\textit{MainEntities}$

The objective is to analyze the paths returned for finding which are the most frequent classes and predicates between them:  
The most frequent via nodes will reinforce/ extend the number of entities.
The edges (dbpedia properties) will determine which are the most relevant properties inside the context of this news item.

The output of this process is two lists: a re-expanded and re-ranked context and a list of the the most important predicates using to link concepts involved in this news item.

\subsubsection{Reranking entities via Filtering}

Get intermediate items using the commonalities function for some top ranked items
\url{http://demo.everythingisconnected.be/commonalities?between=http://dbpedia.org/resource/Lyon&and=http://dbpedia.org/resource/France&allowed=http://purl.org/dc/terms/spatial}
or via a SPARQL query which in this case can be quite performant as well.

\section{Use Case: Snowden Assylum}

Based on the video \url{}. Small excerpt of the story behind: Snowden talks from a Russian airport about his request to have asylum in a country after the extradition from U.S.

Time period:  2013/07/06- 2013/07/17
\subsection {Named Entity Extraction}

Russia	0.809216	Mixed	Country	Location 

Edward Snowden	0.717369	Mixed	Person 

South America	0.56586	Mixed	Continent	Location	

president Putin	0.459811	positive	Person	

president	0.401138	negative	JobTitle		

Federal Migration Service	0.380782	positive	Organization	

Moscow	0.352101	Mixed	City	

CIA	0.334887	neutral	Organization	
	
Bolivia	0.324607	neutral	Country	Location

Obama	0.321901	negative	Person

Missing entities: 
\begin{itemize}
  \item  The lawyer, behind the case (only displayed in OCR but never mentioned.)
  \item  The name of the airport. (There are two airport in Moscow, we don't know which one it is)
  \item  Why there are so many entities concerning South America countries?
\end{itemize}

\subsection {Named Entity Expansion}

Based on the execution: 
\url{curl -X POST --data-binary @snowden.txt "http://localhost:8006/api/rankedentities?startdate=2013-07-16&enddate=2013-08-15&limit=50" --header "Content-Type:text/xml" -v >> snowden_textrazor_50_130716_130815.json2}

   ====  Russia  ====  
0.5715187775678866  0.725645  7  197  4176  http://nerd.eurecom.fr/ontology\#Location http://en.wikipedia.org/wiki/Russia

   ====  Edward Snowden  ====  
0.4857142857142857  0.5  2  242  73  http://nerd.eurecom.fr/ontology\#Person  DBpedia:Person;Freebase:/people/person  

   ====  asylum  ====  
0.2976304037780401  0.388932  4  84  259  http://nerd.eurecom.fr/ontology\#Thing  null  http://en.wikipedia.org/wiki/Right\_of\_asylum

   ====  America  ====  
0.2611293709563164  0.532124  5  50  4034  http://nerd.eurecom.fr/ontology\#Location  http://en.wikipedia.org/wiki/United\_States

   ====  Vladimir Putin  ====  
0.21937027792207794  1.0  1  99  217  http://nerd.eurecom.fr/ontology\#Person http://en.wikipedia.org/wiki/Vladimir\_Putin

   ====  Barack Obama  ====  
0.18996458087367177  0.718052  1  89  234  http://nerd.eurecom.fr/ontology\#Person  http://en.wikipedia.org/wiki/Barack\_Obama

   ====  MOSCOW  ====  
0.1732730106257379  0.560296  1  85  0  http://nerd.eurecom.fr/ontology\#Location http://en.wikipedia.org/wiki/Moscow

   ====  extradition  ====  
0.11783583872491146  0.582043  2  14  1246  http://nerd.eurecom.fr/ontology\#Thing  null  http://en.wikipedia.org/wiki/Extradition

   ====  United States  ====  
0.0986840453364817  0.659101  1  40  --  http://nerd.eurecom.fr/ontology\#Location http://en.wikipedia.org/wiki/United\_States

   ====  human-rights  ====  
0.07678041133412042  0.543733  2  10  3535  http://nerd.eurecom.fr/ontology\#Organization http://en.wikipedia.org/wiki/Human\_rights

   ====  Anatoly Kucherena  ====  
0.06942148760330578  0.5  0  42  --  http://nerd.eurecom.fr/ontology\#Person --

   ====  Kremlin  ====  
0.06620044675324676  0.114493  1  11  446  http://nerd.eurecom.fr/ontology\#Location  Freebase:/location/location  http://en.wikipedia.org/wiki/Kremlin

   ====  Sheremetyevo  ====  
0.03966942148760331  0.356817  0  24  3535  http://nerd.eurecom.fr/ontology\#Location http://en.wikipedia.org/wiki/Sheremetyevo\_International\_Airport

Re- ranked entitites:
\begin{itemize}
  \item  Assylum, the main subject behind this news item.
  \item  Anatoly Kucherena is the lawyer involved in the defense of Edward Snowden.
  \item  The Name of the airport is: 
  \item  Other important entities like "human-rights" and "extradition" are now present. 
\end{itemize}

\subsection {Named Entity Expansion}



\section{Evaluation}

We compare the approach with a set of entities defined by an expert

Entities
1.     Edward Joseph Snowden (M)
2.     Political Asylum (M)
3.     CIA (M)
4.     Sheremetyevo Airport (NM)
5.     Anatoly Kucherena (NM)
6.     Russia (M)
7.     Department of State (NM)


Edward Joseph Snowden: (M)
o   Public figure. He is the “who” of the news.
o   The subject of the main sentence.
 
Political Asylum (M)
o   This is related to the “what” of the news. This is what he requested, the direct object.
 
CIA (M)
o   Background information on related to Snowden, the main subject of the news, “ex CIA employee”.
o   An actor in the news in a wider sense, not this news in particular, but Snowden’s history.
 
Sheremetyevo Airport (NM)
o   Not mentioned by name, specific location of the news.
o   Mentioned as “the Moscow airport”
 
Anatoly Kucherena (NM)
o   Secondary actor and speaker in the video.
o   Information about an interviewee or person expressing his opinion.
 
Rusia (M)
o   The location, but also an actor, the indirect object of the main sentence “to whom”.
 
US Department of State (NM)
o   Involved organization. Not mentioned but related to speaker.
o   Appears in speakers’ title bar.


\label{sec:evaluation}

\section{Conclusions}
We presented a new approach for context-aware connecting news events. Our preliminary results indicate that by exploring DBpedia paths in named entities occuring in news media. ...

%ACKNOWLEDGMENTS are optional
\section{Acknowledgments}
The research activities described in this paper were funded by Ghent University,
%iMinds (Interdisciplinary institute for Technology) a research institute founded by the Flemish Government,
the Institute for the Promotion of Innovation by Science and Technology in Flanders (IWT), the Fund for Scientific Research-Flanders (FWO-Flanders), and the European Union's 7th Framework Programme via the project LinkedTV (GA 287911).

%
% The following two commands are all you need in the
% initial runs of your .tex file to
% produce the bibliography for the citations in your paper.
\bibliographystyle{abbrv}
\bibliography{connectedEvents}  % sigproc.bib is the name of the Bibliography in this case
% You must have a proper ".bib" file
%  and remember to run:
% latex bibtex latex latex
% to resolve all references
%
% ACM needs 'a single self-contained file'!
%

\end{document}
